---
title: About
layout: page
---
![Profile Image]({% if site.external-image %}{{ site.picture }}{% else %}{{ site.url }}/{{ site.picture }}{% endif %})

<p>Akshay K. Jagadish is a PhD student at the Institute for Human Centered AI in Munich and the Max Planck Institute for Biological Cybernetics in Tübingen. His research focuses on understanding the essential ingredients for explaining human adaptive behavior across multiple task domains. To achieve this, he explores themes such as ecological rationality, curriculum learning, and computation under resource constraints. Most recently, he has applied the same principles to study "human-like" adaptive behavior exhibited by foundation models. </p>

<h2>Selected Publications</h2>

<p>
	Binz, M., Dasgupta, I., <b>Jagadish, A. K.</b>, Botvinick, M., Wang, J.X., & Schulz, E. (in press). <a href="https://arxiv.org/abs/2304.06729/">Meta-Learned Models of Cognition</a>. Behavioral and Brain Sciences. <span style="vertical-align:-75%"></span><br>
	<b>Jagadish, A. K.</b>, Coda-Forno, J., Thalmann, M., Schulz, E., & Binz, M., (2024). <a href="https://arxiv.org/abs/2402.01821">Human-like Category Learning by Injecting Ecological Priors from Large Language Models into Neural Networks</a>. Proceedings of the 41st International Conference on Machine Learning (ICML), Vienna, Austria. <span style="vertical-align:-75%"></span><br>
	Schubert, J. A., <b>Jagadish, A. K.</b>, Binz, M., & Schulz, E. (2024). <a href="https://arxiv.org/abs/2402.03969">In-Context Learning Agents Are Asymmetric Belief Updaters</a>. Proceedings of the 41st International Conference on Machine Learning (ICML), Vienna, Austria. <span style="vertical-align:-75%"></span><br>
	<b>Jagadish, A. K.</b>, Binz, M., Saanum, T., Wang, J.X., & Schulz, E. (under review). <a href="https://osf.io/preprints/psyarxiv/ymve5">Zero-shot compositional reasoning in a reinforcement learning setting</a>. <span style="vertical-align:-75%"></span><br>
	Coda-Forno, J., Witte, K., <b>Jagadish, A. K.</b>, Binz, M., & Schulz, E. (under review). <a href="https://arxiv.org/abs/2304.11111">Inducing anxiety in large language models increases exploration and bias</a>. <span style="vertical-align:-75%"></span><br>
</p>

<!-- <li><a href="https://osf.io/preprints/psyarxiv/j7fwb">“Chat-GPT on the Couch”: Mitigating State Anxiety in Large Language Models via Mindfulness-based Relaxation Techniques</a></li> -->
